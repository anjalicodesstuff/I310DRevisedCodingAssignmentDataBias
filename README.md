# I310DRevisedCodingAssignmentDataBias
Coding Assignment: Data Bias, I310D


Coding Assignment: Data Bias, I310D

Step 3: Testing

My hypotheses:

Perspective will be less likely to mark anti-male content as toxic when compared to anti-female content.

Step 4: Results & Analysis

What biases do you think might exist in the model based on intuitions or public documentation about how the model was created? 
I think this model may be biased in terms of measuring what phrases/words are considered toxic. What were your results? I found that the shorter the statements, it was easier for the model to understand/detect toxicity. However, longer comments with more complex insults/phrases tended to be scored lower in terms of toxicity than I would have found. 

Results:
The outcomes from the model did surprise me a bit. Contrary to my initial thought, the model actually leaned more towards marking anti-male content as more toxic than anti-female content. This prompted tne question of what underlying biases within the model existed. In addition, the model was found to have an accuracy of ~50% which definitely couold be improved upon. 

What theories do you have about why your results are what they are? 
I think this model made me consider whether imbalances in the representation of gender-specific language within training data could be skewing the model's findings. Also, we're the model's design, specifically its labeling & limited features, may contribute to the observed discrepancies, highlighting the complexity of biases in ML systems. 

